# VDC MAPP Backend Updates Help
As of writing this document, the backend codebase of the VDC MAPP (previously known as the VDC PPP) project is getting a _major_ overhaul. This document outlines some areas that R&D could use some help developing. 

### Scope: Cosential Data Processing
The functions below are aimed at processing the raw data queried from the Cosential database. 

## Getting Started
In order to test these functions, you should start by importing the "cosential_data_raw.csv" file which contains data queried directly from Cosential with a few columns renamed and a few columns dropped from the original dataset. Once you have this data read in as a pandas dataframe, you can use it as the input to the functions you will [hopefully] develop. Below is a suggested skeleton for your python script to develop and test these functions:

```python
import pandas as pd

# Function 1

# Function 2

# Function 3

# Function 4

def main():
    # Read data from a CSV file
    data = pd.DataFrame() # edit this line

    # Display original data
    
    # Call functions sequentially

if __name__ == "__main__":
    main()
```

# Functions
Below are four functions that R&D needs help developing

## Drop Duplicate Rows
_Drop duplicate rows from the input dataframe `df` based on the values in the specified column `column_name`_

### Inputs
* `df`: The pandas DataFrame containing the data.
* `column_name`: The name of the column based on which duplicates will be dropped.

### Outputs
_Use `print` statements for these_
* Output the value in the input column each time a duplicate row is dropped.
* Keep a count of all the rows that have been dropped and output the number at the end.

### Returns
* The original dataframe with the duplicate rows dropped.

### Current Implementation
Currently, no function is used. A single one-line expression using the built-in `drop_duplicates` method is used. This method works but provides no output which I need to help diagnose other issues. Also the `inplace` input is being deprecated in later released of `pandas`.

```python
data.drop_duplicates(
    subset=["Project"],
    inplace=True
)
```

### New Implementation
Below is the suggested skeleton for this function:

```python
def drop_duplicate_rows(df, column_name):
    """
    Drops duplicate rows from the input DataFrame based on the specified column.

    Parameters
    ----------
    df : DataFrame
        The pandas DataFrame containing the data.
    column_name : str
        The name of the column based on which duplicates will be dropped.

    Returns
    -------
    DataFrame
        The original dataframe with duplicate rows dropped.
    """
    # Function implementation goes here
    pass
```

### Difficulty and Suggestions
:red_square: **Hard**: While this function seems straight-forward, it is complicated by the fact that I want to output one of the values in the row that is dropped. So instead of using the built-in `drop_duplicates()` method, you will have to iterate through each row - at least I think you would...

## Convert Specified Date Columns to String
_Standardize datetime columns to string representations_

### Inputs
* `df`: The pandas DataFrame containing the data.
* `column_names`: list of strings corresponding to the datetime columns that need standardization.
* `datetime_format`: string format representation of the date, default should be `"%m/%d/%Y"`.

### Outputs
None

### Returns
* A copy of the original dataframe but with the specified columns updated

### Current Implementation
Again, no function just a loop iterating through the columns I want updated. 

```python
for col in ["InterviewDate", "BudgetDate", "ConstructionStart", "BidDocsAvailable"]:
    data[col] = pd.to_datetime(data[col]) # convert to datetime first so there are no TypeErrors
    data[col] = data[col].dt.strftime("%m/%d/%Y")
```

### New Implementation
Below is the suggested skeleton for this function:

```python
def convert_datetime_cols_to_str(dataframe, column_names, datetime_format="%m/%d/%Y"):
    """
    Standardize datetime columns to string representations.

    Parameters
    ----------
    dataframe : DataFrame
        The pandas DataFrame containing the data.
    column_names : list of str
        List of strings corresponding to the datetime columns that need standardization.
    datetime_format : str, optional
        String format representation of the date, default is "%m/%d/%Y".

    Returns
    -------
    DataFrame
        A copy of the original dataframe but with the specified columns updated.
    """
    # Function implementation goes here
    pass
```

### Difficulty and Suggestions
:green_square: **Easy**: This function should be more straight-forward, especially since you can use the previous implementation almost exactly. These columns already have the date as a string, but the format varies. That is why the function should first convert the original string to a datetime and then convert it _back_ to a string. Be sure to start the function by creating a copy of the input dataframe so you are not modifying the original.

## Create "Update" Columns
_Create boolean "watcher" columns for specified variables to know when that variable has been updated_

### Inputs
* `df`: The pandas DataFrame containing the data.
* `column_names`: list of strings corresponding to the columns that need watcher/update columns associated with them.

### Outputs
None

### Returns
A copy of the original dataframe but with new watcher columns.

### Current Implementation
No function, but a loop that creates the column and sets the default value to False:

```python
columns_to_watch = ["Phase", "Budget", "InterviewDate", "BudgetDate", "ConstructionStart"]
for col in columns_to_watch:
    data[f"{col}Update"] = False
```

### New Implementation

```python
def create_update_columns(df, column_names):
    """
    Create boolean "watcher" columns for specified variables to know when that variable has been updated.

    Parameters
    ----------
    df : DataFrame
        The pandas DataFrame containing the data.
    column_names : list of str
        List of strings corresponding to the columns that need watcher/update columns associated with them.

    Returns
    -------
    DataFrame
        A copy of the original dataframe but with new watcher columns.
    """
    # Function implementation goes here
    pass
```

### Difficulty and Suggestions
:green_square: **Easy**: This function should look very similar to the previous implementation but in function form with input variables replacing the others. Be sure to set all rows of the newly created columns to `False`. Be sure to start the function by creating a copy of the input dataframe so you are not modifying the original.

## Handle NaN/Null Values
_Systematically fill in null values for various columns_

### Inputs
* `df`: The pandas DataFrame containing the data.

### Outputs
* Number of null values per column before being filled in.

### Returns
A copy of the original dataframe with the null values replaced.

### Current Implementation
No function, but a few lines without outputs:

```python
# string NaNs
for column in ["InterviewDate","BidDocsAvailable","BidDocsAvailable","BudgetDate","ConstructionStart", "ProjectID"]:
    data[column] = data[column].fillna("Not Specified")
data["PreconLead"] = data["PreconLead"].fillna("Not Assigned")
# numerical NaNs
for column in ["EstimatedSF", "Duration"]:
    data[column].fillna(0)
    
data["ProjectID"] = data["ProjectID"].apply(lambda val: "Not Specified" if len(val) < 2 else val)
```

### New Implementation
Here is the suggested skeleton of the new implementation:

```python
def handle_nan_values(df):
    """
    Systematically fill in null values for various columns.

    Parameters
    ----------
    df : DataFrame
        The pandas DataFrame containing the data.

    Returns
    -------
    DataFrame
        A copy of the original dataframe with the null values replaced.

    """
    # Function implementation goes here
    pass
```

### Difficulty and Suggestions
:yellow_square: **Medium**: This function is slightly more complicated since I am asking you to output the number of null values that are to be replaced. Be sure to create a copy of the dataframe before you start. There is not going to be a streamlined, generic way to handle the various scenarios you see in the original implementation so feel free to use hard-coded column names. Also, please don't replace the null `PreconLead` values with "Not Assigned" - use "Not Specified". 
